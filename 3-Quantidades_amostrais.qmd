---
title: "Quantidades amostrais"
title-slide-attributes:
  data-background-image: Marca_UEPB.png
  data-background-size: contain
  data-background-opacity: "0.1"
author: "Pedro Almeida"
format: 
  revealjs: 
    logo: Marca_UEPB.png
    theme: solarized
    transition: slide
    background-transition: fade
    smaller: false
    scrollable: true
knitr:
  opts_chunk:
    message: false
    warning: false
css: hscroll.css
---

::: {style="height:500px; font-size:30px; margin-left: 5px; margin-right: 5px"}

# Amostras aleatórias 

- Para estudar a variabilidade amostral de estatísticas como $\overline{\mathbf{x}}$ e $\mathbf{S}_n$ com o objetivo final de fazer inferências, precisamos fazer suposições sobre as variáveis cujos valores observados constituem o conjunto de dados $\mathbf{X}$.

- Suponha, então, que os dados ainda não tenham sido observados, mas pretendemos coletar uma amostra com $n$ indivíduos em $p$ variáveis. Neste contexto, seja a $(j, k)$-ésima entrada na matriz de dados a variável aleatória $X_{j k}$. Cada indivíduo da amostra, $\mathbf{X}_j$ nas variáveis $p$, é um vetor aleatório e temos a matriz aleatória

$$
\underset{(n \times p)}{\mathbf{X}}=\left[\begin{array}{cccc}
X_{11} & X_{12} & \cdots & X_{1p} \\
X_{21} & X_{22} & \cdots & X_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
X_{n 1} & X_{n 2} & \cdots & X_{n p}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{X}_1^{\prime} \\
\mathbf{X}_2^{\prime} \\
\vdots \\
\mathbf{X}_n^{\prime}
\end{array}\right]
$$

:::


## Função de densidade conjunta para vetores aleatórios

Se os vetores linha $\mathbf{X}_1^{\prime}, \mathbf{X}_2^{\prime}, \ldots, \mathbf{X}_n^{\prime}$ representam observações independentes de uma distribuição conjunta comum com função de densidade $f(\mathbf{x})=f\left(x_1, x_2, \ldots, x_p\right)$, então $\mathbf{X}_1, \mathbf{ ; X}_2, \ldots, \mathbf{X}_n$ formam uma amostra aleatória de $f(\mathbf{x})$. Matematicamente, $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ formam uma amostra aleatória se sua função de densidade conjunta for dada pelo produto $f\left(\mathbf{x ). }_1\right) f \left(\mathbf{x}_2\right) \cdots f\left(\mathbf{x}_n\right)$, onde $f\left(\mathbf{x}_j \right) =f\left(x_{j1},x_{j2},\ldots,x_{jp}\right)$ é a função de densidade para o $j$ o vetor linha.

## Observações da definição de amostra aleatória

1. As medições das variáveis $p$ em uma única tentativa, como $\mathbf{X}_j^{\prime}=$ $\left[X_{j 1}, X_{j 2}, \ldots, X_{j p}\right]$, geralmente serão correlacionados. De fato, esperamos que assim seja. As medições de diferentes ensaios devem, no entanto, ser independentes.


2. A independência das medições de tentativa para tentativa pode não se manter quando as variáveis tendem a se desviar ao longo do tempo, como com conjuntos de preços de ações $p$ ou indicadores econômicos $p$. Violações do pressuposto provisório de independência podem ter um sério impacto na qualidade das inferências estatísticas.

## Implicações da independência para calcular distâncias

A noção de independência estatística tem implicações importantes para medir a distância. 

- A distância euclidiana parece apropriada se os componentes de um vetor forem independentes e tiverem as mesmas variações. 
- Suponha que consideramos a localização da $k$-ésima coluna $\mathbf{Y}_k^{\prime}=\left[X_{1 k}, X_{2 k}, \ldots, X_{n k}\right]$ de $\mathbf{X}$, considerado como um ponto em $n$ dimensões. 

- A localização deste ponto é determinada pela distribuição de probabilidade conjunta $f \left(\mathbf{y}_k\right)=f\left(x_{1 k}, x_{2 k}, \ldots, x_{n k} \right)$. Quando as medições $X_{1 k}, X_{2 k}, \ldots, X_{n k}$ são uma amostra aleatória, $f\left(\mathbf{y}_k\right)=f\left(x_{ 1 k}, x_{2 k}, \ldots, x_{n k}\right)=$ $f_k\left(x_{1 k}\right) f_k\left(x_{2 k}\right) \cdots f_k \left(x_{n k}\right)$ e, consequentemente, cada coordenada $x_{j k}$ contribui igualmente para a localização através das distribuições marginais idênticas $f_k\left(x_{j k}\right)$.


## Cenário com as variáveis não são independentes

- Se os componentes $n$ não forem independentes ou as distribuições marginais não forem idênticas, a influência das medições individuais (coordenadas) na localização é assimétrica. Seríamos então levados a considerar uma função de distância na qual as coordenadas eram ponderadas de forma desigual, como nas distâncias "estatísticas".

- Certas conclusões podem ser alcançadas sobre as distribuições amostrais de $\overline{\mathbf{X}}$ e $\mathbf{S}_n$ sem fazer outras suposições sobre a forma da distribuição conjunta subjacente das variáveis. Em particular, podemos ver como $\overline{\mathbf{X}}$ e $\mathbf{S}_n$ se saem como estimadores pontuais do vetor médio populacional correspondente $\boldsymbol{\mu}$ e matriz de covariância $\boldsymbol{\Sigma}$.


## Resultados Amostrais

Seja $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ uma amostra aleatória de uma distribuição conjunta que tem vetor de médias $\boldsymbol{\mu}$ e matriz de covariância $\boldsymbol{\Sigma}$. Então $\overline{\mathbf{X}}$ é um estimador não viesado de $\boldsymbol{\mu}$, e a matriz de covariância é
$$
\frac{1}{n} \Sigma
$$
Então,
$$
\begin{aligned}
& E(\overline{\mathbf{X}})=\boldsymbol{\mu} \quad \text { (vetor de médias populacionais) } \\
& \operatorname{Cov}(\overline{\mathbf{X}})=\frac{1}{n} \Sigma \quad\left(\begin{array}{c}
\text { matriz de covariância populacional } \\
\text { dividido pelo tamanho da amostra }
\end{array}\right) \\
&
\end{aligned}
$$
Para a matriz de covariância $\mathbf{S}_n$
$$
E\left(\mathbf{S}_n\right)=\frac{n-1}{n} \boldsymbol{\Sigma}=\mathbf{\Sigma}-\frac{1}{n} \mathbf{\Sigma}
$$
Então,
$$
E\left(\frac{n}{n-1} \mathbf{S}_n\right)=\Sigma
$$
em que $[n /(n-1)] \mathbf{S}_n$ é um estimador não viesado de $\mathbf{\Sigma}$, equanto $\mathbf{S}_n$ é um estimador viesado com (Viés)=$E\left(\mathbf{S}_n\right)-\mathbf{\Sigma}=-(1 / n) \mathbf{\Sigma}$.




